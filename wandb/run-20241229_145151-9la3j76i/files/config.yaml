_wandb:
    value:
        cli_version: 0.19.1
        m: []
        python_version: 3.12.8
        t:
            "1":
                - 1
                - 41
                - 55
            "2":
                - 1
                - 41
                - 55
            "3":
                - 13
                - 16
                - 23
                - 55
            "4": 3.12.8
            "5": 0.19.1
            "8":
                - 5
            "12": 0.19.1
            "13": linux-x86_64
always_save_checkpoint:
    value: false
backend:
    value: nccl
batch_size:
    value: 12
beta1:
    value: 0.9
beta2:
    value: 0.99
bias:
    value: false
block_size:
    value: 224
compile:
    value: false
config_string:
    value: nl7_ed576_nh12121212121212_mr4444444
decay_lr:
    value: true
device:
    value: cuda
dropout:
    value: 0.2
dtype:
    value: float16
eval_interval:
    value: 100
eval_iters:
    value: 100
eval_only:
    value: false
grad_clip:
    value: 1
gradient_accumulation_steps:
    value: 20
init_from:
    value: scratch
k:
    value: mlp_ratio
k_:
    value: mr
learning_rate:
    value: 0.0005
log_interval:
    value: 10
max_iters:
    value: 12000
min_lr:
    value: 5e-05
n_embd:
    value: 576
n_layer:
    value: 7
out_dir:
    value: output_tinystories/out_train_None_None_nl7_ed576_nh12121212121212_mr4444444_maxiter12000_9001_20241229-145150
timestr:
    value: 20241229-145150
v_:
    value: "4444444"
wandb_log:
    value: true
wandb_project:
    value: tinystories-gpt2enc
wandb_run_name:
    value: gpt2
warmup_iters:
    value: 100
weight_decay:
    value: 0.1
